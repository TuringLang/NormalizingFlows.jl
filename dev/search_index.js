var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = NormalizingFlows","category":"page"},{"location":"#NormalizingFlows","page":"Home","title":"NormalizingFlows","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NormalizingFlows.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [NormalizingFlows]","category":"page"},{"location":"#NormalizingFlows.grad!-Tuple{Random.AbstractRNG, ADTypes.AbstractADType, Any, AbstractVector{<:Real}, Any, DiffResults.MutableDiffResult, Vararg{Any}}","page":"Home","title":"NormalizingFlows.grad!","text":"grad!(\n    rng::AbstractRNG,\n    ad::ADTypes.AbstractADType,\n    vo,\n    θ_flat::AbstractVector{<:Real},\n    reconstruct,\n    out::DiffResults.MutableDiffResult,\n    args...\n)\n\nCompute the value and gradient for negation of the variational objective vo  at θ_flat using the automatic differentiation backend ad.  \n\nDefault implementation is provided for ad where ad is one of AutoZygote,  AutoForwardDiff, AutoReverseDiff (with no compiled tape), and AutoEnzyme. The result is stored in out.\n\nArguments\n\nrng::AbstractRNG: random number generator\nad::ADTypes.AbstractADType: automatic differentiation backend\nvo: variational objective\nθ_flat::AbstractVector{<:Real}: flattened parameters of the normalizing flow\nreconstruct: function that reconstructs the normalizing flow from the flattened parameters\nout::DiffResults.MutableDiffResult: mutable diff result to store the value and gradient\nargs...: additional arguments for vo\n\n\n\n\n\n","category":"method"},{"location":"#NormalizingFlows.loglikelihood-Tuple{Bijectors.UnivariateTransformed, AbstractVector}","page":"Home","title":"NormalizingFlows.loglikelihood","text":"loglikelihood(flow::Bijectors.TransformedDistribution, xs::AbstractVecOrMat)\n\nCompute the log-likelihood for variational distribution flow at a batch of samples xs from  the target distribution.\n\n\n\n\n\n","category":"method"},{"location":"#NormalizingFlows.optimize-Tuple{Random.AbstractRNG, ADTypes.AbstractADType, Any, AbstractVector{<:Real}, Any, Vararg{Any}}","page":"Home","title":"NormalizingFlows.optimize","text":"optimize(\n    rng::AbstractRNG, \n    ad::ADTypes.AbstractADType, \n    vo, \n    θ₀::AbstractVector{T}, \n    re, \n    args...; \n    kwargs...\n)\n\nIteratively updating the parameters θ of the normalizing flow re(θ) by calling grad!  and using the given optimiser to compute the steps.\n\nArguments\n\nrng::AbstractRNG: random number generator\nad::ADTypes.AbstractADType: automatic differentiation backend\nvo: variational objective\nθ₀::AbstractVector{T}: initial parameters of the normalizing flow\nre: function that reconstructs the normalizing flow from the flattened parameters\nargs...: additional arguments for vo\n\nKeyword Arguments\n\nmax_iters::Int=10000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nshow_progress::Bool=true: whether to show the progress bar. The default information printed in the progress bar is the iteration number, the loss value, and the gradient norm.\ncallback=nothing: callback function with signature cb(iter, opt_state, re, θ) which returns a dictionary-like object of statistics to be displayed in the progress bar. re and θ are used for reconstructing the normalizing flow in case that user  want to further axamine the status of the flow.\nprog=ProgressMeter.Progress(           max_iters; desc=\"Training\", barlen=31, showspeed=true, enabled=show_progress       ): progress bar configuration\n\nReturns\n\nθ: trained parameters of the normalizing flow\nopt_stats: statistics of the optimiser\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"method"},{"location":"#NormalizingFlows.train_flow-Tuple{Any, Any, Vararg{Any}}","page":"Home","title":"NormalizingFlows.train_flow","text":"train_flow([rng::AbstractRNG, ]vo, flow, args...; kwargs...)\n\nTrain the given normalizing flow flow by calling optimize.\n\nArguments\n\nrng::AbstractRNG: random number generator\nvo: variational objective\nflow: normalizing flow to be trained\nargs...: additional arguments for vo\n\nKeyword Arguments\n\nmax_iters::Int=1000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nADbackend::ADTypes.AbstractADType=ADTypes.AutoZygote(): automatic differentiation backend\nkwargs...: additional keyword arguments for optimize (See optimize)\n\nReturns\n\nflow_trained: trained normalizing flow\nopt_stats: statistics of the optimiser during the training process (See optimize)\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"method"},{"location":"#NormalizingFlows.value_and_gradient!","page":"Home","title":"NormalizingFlows.value_and_gradient!","text":"value_and_gradient!(\n    ad::ADTypes.AbstractADType,\n    f,\n    θ::AbstractVector{T},\n    out::DiffResults.MutableDiffResult\n) where {T<:Real}\n\nCompute the value and gradient of a function f at θ using the automatic differentiation backend ad.  The result is stored in out.  The function f must return a scalar value. The gradient is stored in out as a vector of the same length as θ.\n\n\n\n\n\n","category":"function"}]
}
