<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · NormalizingFlows.jl</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>NormalizingFlows.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#What-are-normalizing-flows?"><span>What are normalizing flows?</span></a></li></ul></li><li><a class="tocitem" href="api/">API</a></li><li><a class="tocitem" href="example/">Example</a></li><li><a class="tocitem" href="customized_layer/">Customize your own flow layer</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/TuringLang/NormalizingFlows.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="NormalizingFlows.jl"><a class="docs-heading-anchor" href="#NormalizingFlows.jl">NormalizingFlows.jl</a><a id="NormalizingFlows.jl-1"></a><a class="docs-heading-anchor-permalink" href="#NormalizingFlows.jl" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/TuringLang/NormalizingFlows.jl">NormalizingFlows</a>.</p><p>The purpose of this package is to provide a simple and flexible interface for  variational inference (VI) and normalizing flows (NF) for Bayesian computation and generative modeling. The key focus is to ensure modularity and extensibility, so that users can easily  construct (e.g., define customized flow layers) and combine various components  (e.g., choose different VI objectives or gradient estimates)  for variational approximation of general target distributions,  <em>without being tied to specific probabilistic programming frameworks or applications</em>. </p><p>See the <a href="https://turinglang.org/NormalizingFlows.jl/dev/">documentation</a> for more.  </p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>To install the package, run the following command in the Julia REPL:</p><pre><code class="nohighlight hljs">]  # enter Pkg mode
(@v1.9) pkg&gt; add git@github.com:TuringLang/NormalizingFlows.jl.git</code></pre><p>Then simply run the following command to use the package:</p><pre><code class="language-julia hljs">using NormalizingFlows</code></pre><h2 id="What-are-normalizing-flows?"><a class="docs-heading-anchor" href="#What-are-normalizing-flows?">What are normalizing flows?</a><a id="What-are-normalizing-flows?-1"></a><a class="docs-heading-anchor-permalink" href="#What-are-normalizing-flows?" title="Permalink"></a></h2><p>Normalizing flows transform a simple reference distribution <span>$q_0$</span> (sometimes known as base distribution) to  a complex distribution <span>$q_\theta$</span> using invertible functions with trainable parameter <span>$\theta$</span>, aiming to approximate a target distribution <span>$p$</span>. The approximation is achieved by minimizing some statistical distances between <span>$q$</span> and <span>$p$</span>.</p><p>In more details, given the base distribution, usually a standard Gaussian distribution, i.e., <span>$q_0 = \mathcal{N}(0, I)$</span>, we apply a series of parameterized invertible transformations (called flow layers), <span>$T_{1, \theta_1}, \cdots, T_{N, \theta_k}$</span>, yielding that</p><p class="math-container">\[Z_N = T_{N, \theta_N} \circ \cdots \circ T_{1, \theta_1} (Z_0) , \quad Z_0 \sim q_0,\quad  Z_N \sim q_{\theta}, \]</p><p>where <span>$\theta = (\theta_1, \dots, \theta_N)$</span> are the parameters to be learned, and <span>$q_{\theta}$</span> is the transformed distribution (typically called the variational distribution or the flow distribution).  This describes <strong>sampling procedure</strong> of normalizing flows, which requires sending draws from the base distribution through a forward pass of these flow layers.</p><p>Since all the transformations are invertible (technically <a href="https://en.wikipedia.org/wiki/Diffeomorphism">diffeomorphic</a>),  we can evaluate the density of a normalizing flow distribution <span>$q_{\theta}$</span> by the change of variable formula: </p><p class="math-container">\[q_\theta(x)=\frac{q_0\left(T_1^{-1} \circ \cdots \circ
T_N^{-1}(x)\right)}{\prod_{n=1}^N J_n\left(T_n^{-1} \circ \cdots \circ
T_N^{-1}(x)\right)} \quad J_n(x)=\left|\operatorname{det} \nabla_x
T_n(x)\right|.\]</p><p>Here we drop the subscript <span>$\theta_n, n = 1, \dots, N$</span> for simplicity.  Density evaluation of normalizing flow requires computing the <strong>inverse</strong> and the <strong>Jacobian determinant</strong> of each flow layer.</p><p>Given the feasibility of i.i.d. sampling and density evaluation, normalizing flows can be trained by minimizing some statistical distances to the target distribution <span>$p$</span>. The typical choice of the statistical distance is the forward and reverse Kullback-Leibler (KL) divergence, which leads to the following optimization problems:</p><p class="math-container">\[\begin{aligned}
\text{Reverse KL:}\quad
&amp;\argmin _{\theta} \mathbb{E}_{q_{\theta}}\left[\log q_{\theta}(Z)-\log p(Z)\right] \\
&amp;= \argmin _{\theta} \mathbb{E}_{q_0}\left[\log \frac{q_\theta(T_N\circ \cdots \circ T_1(Z_0))}{p(T_N\circ \cdots \circ T_1(Z_0))}\right] \\
&amp;= \argmax _{\theta} \mathbb{E}_{q_0}\left[ \log p\left(T_N \circ \cdots \circ T_1(Z_0)\right)-\log q_0(X)+\sum_{n=1}^N \log J_n\left(F_n \circ \cdots \circ F_1(X)\right)\right]
\end{aligned}\]</p><p>and </p><p class="math-container">\[\begin{aligned}
\text{Forward KL:}\quad
&amp;\argmin _{\theta} \mathbb{E}_{p}\left[\log q_{\theta}(Z)-\log p(Z)\right] \\
&amp;= \argmin _{\theta} \mathbb{E}_{p}\left[\log q_\theta(Z)\right] 
\end{aligned}\]</p><p>Both problems can be solved via standard stochastic optimization algorithms, such as stochastic gradient descent (SGD) and its variants. </p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Wednesday 23 August 2023 11:39">Wednesday 23 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
