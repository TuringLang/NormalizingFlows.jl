var documenterSearchIndex = {"docs":
[{"location":"customized_layer/#Defining-Your-Own-Flow-Layer","page":"Customize your own flow layer","title":"Defining Your Own Flow Layer","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"In practice, user might want to define their own normalizing flow.  As briefly noted in What are normalizing flows?, the key is to define a customized normalizing flow layer, including its transformation and inverse, as well as the log-determinant of the Jacobian of the transformation. Bijectors.jl offers a convenient interface to define a customized bijection. We refer users to the documentation of Bijectors.jl for more details. Flux.jl is also a useful package, offering a convenient interface to define neural networks.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"In this tutorial, we demonstrate how to define a customized normalizing flow layer – an Affine Coupling Layer – using Bijectors.jl and Flux.jl, which is the building block of the RealNVP flow [LJS2017]. It's worth mentioning that the realnvp implemented in NormalizingFlows.jl is slightly different from this tutorial with some optimization for the training stability and performance.","category":"page"},{"location":"customized_layer/#Affine-Coupling-Flow","page":"Customize your own flow layer","title":"Affine Coupling Flow","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Given an input vector boldsymbolx, the general coupling transformation splits it into two parts: boldsymbolx_I_1 and boldsymbolx_Isetminus I_1. Only one part (e.g., boldsymbolx_I_1) undergoes a bijective transformation f, noted as the coupling law,  based on the values of the other part (e.g., boldsymbolx_Isetminus I_1), which remains unchanged. ","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"beginarrayllll\nc_I_1(cdot  f theta)  mathbbR^d rightarrow mathbbR^d  c_I_1^-1(cdot  f theta)  mathbbR^d rightarrow mathbbR^d \n boldsymbolx_I backslash I_1 mapsto boldsymbolx_I backslash I_1   boldsymboly_I backslash I_1 mapsto boldsymboly_I backslash I_1 \n boldsymbolx_I_1 mapsto fleft(boldsymbolx_I_1  thetaleft(boldsymbolx_Isetminus I_1right)right)   boldsymboly_I_1 mapsto f^-1left(boldsymboly_I_1  thetaleft(boldsymboly_Isetminus I_1right)right)\nendarray","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Here theta can be an arbitrary function, e.g., a neural network. As long as f(cdot theta(boldsymbolx_Isetminus I_1)) is invertible, c_I_1 is invertible, and the  Jacobian determinant of c_I_1 is easy to compute:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"lefttextdet nabla_x c_I_1(x)right = lefttextdet nabla_x_I_1 f(x_I_1 theta(x_Isetminus I_1))right","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"The affine coupling layer is a special case of the coupling transformation, where the coupling law f is an affine function:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"beginaligned\nboldsymbolx_I_1 mapsto boldsymbolx_I_1 odot sleft(boldsymbolx_Isetminus I_1right) + tleft(boldsymbolx_I setminus I_1right) \nboldsymbolx_I backslash I_1 mapsto boldsymbolx_I backslash I_1\nendaligned","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Here, s and t are arbitrary functions (often neural networks) called the \"scaling\" and \"translation\" functions, respectively.  They produce vectors of the same dimension as boldsymbolx_I_1.","category":"page"},{"location":"customized_layer/#Implementing-Affine-Coupling-Layer","page":"Customize your own flow layer","title":"Implementing Affine Coupling Layer","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"We start by defining a simple 3-layer multi-layer perceptron (MLP) using Flux.jl,  which will be used to define the scaling s and translation functions t in the affine coupling layer.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Flux\n\nfunction MLP_3layer(input_dim::Int, hdims::Int, output_dim::Int; activation=Flux.leakyrelu)\n    return Chain(\n        Flux.Dense(input_dim, hdims, activation),\n        Flux.Dense(hdims, hdims, activation),\n        Flux.Dense(hdims, output_dim),\n    )\nend","category":"page"},{"location":"customized_layer/#Construct-the-Object","page":"Customize your own flow layer","title":"Construct the Object","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Following the user interface of Bijectors.jl, we define a struct AffineCoupling as a subtype of Bijectors.Bijector. The functions parition , combine are used to partition and recombine a vector into 3 disjoint subvectors.  And PartitionMask is used to store this partition rule.  These three functions are all defined in Bijectors.jl; see the documentaion for more details.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Functors\nusing Bijectors\nusing Bijectors: partition, combine, PartitionMask\n\nstruct AffineCoupling <: Bijectors.Bijector\n    dim::Int\n    mask::Bijectors.PartitionMask\n    s::Flux.Chain\n    t::Flux.Chain\nend\n\n# to apply functions to the parameters that are contained in AffineCoupling.s and AffineCoupling.t, \n# and to re-build the struct from the parameters, we use the functor interface of `Functors.jl` \n# see https://fluxml.ai/Flux.jl/stable/models/functors/#Functors.functor\n@functor AffineCoupling (s, t)\n\nfunction AffineCoupling(\n    dim::Int,  # dimension of input\n    hdims::Int, # dimension of hidden units for s and t\n    mask_idx::AbstractVector, # index of dimension that one wants to apply transformations on\n)\n    cdims = length(mask_idx) # dimension of parts used to construct coupling law\n    s = MLP_3layer(cdims, hdims, cdims)\n    t = MLP_3layer(cdims, hdims, cdims)\n    mask = PartitionMask(dim, mask_idx)\n    return AffineCoupling(dim, mask, s, t)\nend","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"By default, we define s and t using the MLP_3layer function, which is a 3-layer MLP with leaky ReLU activation function.","category":"page"},{"location":"customized_layer/#Implement-the-Forward-and-Inverse-Transformations","page":"Customize your own flow layer","title":"Implement the Forward and Inverse Transformations","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"function Bijectors.transform(af::AffineCoupling, x::AbstractVector)\n    # partition vector using 'af.mask::PartitionMask`\n    x₁, x₂, x₃ = partition(af.mask, x)\n    y₁ = x₁ .* af.s(x₂) .+ af.t(x₂)\n    return combine(af.mask, y₁, x₂, x₃)\nend\n\nfunction Bijectors.transform(iaf::Inverse{<:AffineCoupling}, y::AbstractVector)\n    af = iaf.orig\n    # partition vector using `af.mask::PartitionMask`\n    y_1, y_2, y_3 = partition(af.mask, y)\n    # inverse transformation\n    x_1 = (y_1 .- af.t(y_2)) ./ af.s(y_2)\n    return combine(af.mask, x_1, y_2, y_3)\nend","category":"page"},{"location":"customized_layer/#Implement-the-Log-determinant-of-the-Jacobian","page":"Customize your own flow layer","title":"Implement the Log-determinant of the Jacobian","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Notice that here we wrap the transformation and the log-determinant of the Jacobian into a single function, with_logabsdet_jacobian.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"function Bijectors.with_logabsdet_jacobian(af::AffineCoupling, x::AbstractVector)\n    x_1, x_2, x_3 = Bijectors.partition(af.mask, x)\n    y_1 = af.s(x_2) .* x_1 .+ af.t(x_2)\n    logjac = sum(log ∘ abs, af.s(x_2))\n    return combine(af.mask, y_1, x_2, x_3), logjac\nend\n\nfunction Bijectors.with_logabsdet_jacobian(\n    iaf::Inverse{<:AffineCoupling}, y::AbstractVector\n)\n    af = iaf.orig\n    # partition vector using `af.mask::PartitionMask`\n    y_1, y_2, y_3 = partition(af.mask, y)\n    # inverse transformation\n    x_1 = (y_1 .- af.t(y_2)) ./ af.s(y_2)\n    logjac = -sum(log ∘ abs, af.s(y_2))\n    return combine(af.mask, x_1, y_2, y_3), logjac\nend","category":"page"},{"location":"customized_layer/#Construct-Normalizing-Flow","page":"Customize your own flow layer","title":"Construct Normalizing Flow","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Now with all the above implementations, we are ready to use the AffineCoupling layer for normalizing flow  by applying it to a base distribution q_0.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Random, Distributions, LinearAlgebra\ndim = 4\nhdims = 10\nLs = [\n    AffineCoupling(dim, hdims, 1:2), \n    AffineCoupling(dim, hdims, 3:4), \n    AffineCoupling(dim, hdims, 1:2), \n    AffineCoupling(dim, hdims, 3:4), \n    ]\nts = reduce(∘, Ls)\nq₀ = MvNormal(zeros(Float32, dim), I)\nflow = Bijectors.transformed(q₀, ts)","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"We can now sample from the flow:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"x = rand(flow, 10)","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"And evaluate the density of the flow:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"logpdf(flow, x[:,1])","category":"page"},{"location":"customized_layer/#Reference","page":"Customize your own flow layer","title":"Reference","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"[LJS2017]: Dinh, L., Sohl-Dickstein, J. and Bengio, S., 2017. Density estimation using real nvp. in ICLR","category":"page"},{"location":"NSF/#Demo-of-NSF-on-2D-Banana-Distribution","page":"Neural Spline Flow","title":"Demo of NSF on 2D Banana Distribution","text":"","category":"section"},{"location":"NSF/","page":"Neural Spline Flow","title":"Neural Spline Flow","text":"using Random, Distributions, LinearAlgebra\nusing Functors\nusing Optimisers, ADTypes\nusing Zygote\nusing NormalizingFlows\n\n\ntarget = Banana(2, one(T), 100one(T))\nlogp = Base.Fix1(logpdf, target)\n\n######################################\n# learn the target using Neural Spline Flow\n######################################\n@leaf MvNormal\nq0 = MvNormal(zeros(T, 2), I)\n\n\nflow = nsf(q0; paramtype=T)\nflow_untrained = deepcopy(flow)\n######################################\n# start training\n######################################\nsample_per_iter = 64\n\n# callback function to log training progress\ncb(iter, opt_stats, re, θ) = (sample_per_iter=sample_per_iter,ad=adtype)\n# nsf only supports AutoZygote\nadtype = ADTypes.AutoZygote()\ncheckconv(iter, stat, re, θ, st) = stat.gradient_norm < one(T)/1000\nflow_trained, stats, _ = train_flow(\n    elbo_batch,\n    flow,\n    logp,\n    sample_per_iter;\n    max_iters=10,   # change to larger number of iterations (e.g., 50_000) for better results\n    optimiser=Optimisers.Adam(1e-4),\n    ADbackend=adtype,\n    show_progress=true,\n    callback=cb,\n    hasconverged=checkconv,\n)\nθ, re = Optimisers.destructure(flow_trained)\nlosses = map(x -> x.loss, stats)","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#Variational-Objectives","page":"API","title":"Variational Objectives","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"We provide ELBO (reverse KL) and expected log-likelihood (forward KL). You can also supply your own objective with the signature vo(rng, flow, args...).","category":"page"},{"location":"api/#Evidence-Lower-Bound-(ELBO)","page":"API","title":"Evidence Lower Bound (ELBO)","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"By maximizing the ELBO, it is equivalent to minimizing the reverse KL divergence between q_theta and p:","category":"page"},{"location":"api/","page":"API","title":"API","text":"beginaligned\nmin _theta mathbbE_q_thetaleftlog q_theta(Z)-log p(Z)right  quad text(Reverse KL)\n = max _theta mathbbE_q_0left log pleft(T_N circ cdots circ\nT_1(Z_0)right)-log q_0(X)+sum_n=1^N log J_nleft(T_n circ cdots circ\nT_1(X)right)right quad text(ELBO)\nendaligned","category":"page"},{"location":"api/","page":"API","title":"API","text":"Reverse KL minimization is typically used for Bayesian computation when only logp is available.","category":"page"},{"location":"api/#NormalizingFlows.elbo","page":"API","title":"NormalizingFlows.elbo","text":"elbo(flow, logp, xs)\nelbo([rng, ] flow, logp, n_samples)\n\nMonte Carlo estimates of the ELBO from a batch of samples xs from the  reference distribution flow.dist.\n\nArguments\n\nrng: random number generator\nflow: variational distribution to be trained. In particular  flow = transformed(q₀, T::Bijectors.Bijector),  q₀ is a reference distribution that one can easily sample and compute logpdf\nlogp: log-pdf of the target distribution (not necessarily normalized)\nxs: samples from reference dist q₀\nn_samples: number of samples from reference dist q₀\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.elbo_batch","page":"API","title":"NormalizingFlows.elbo_batch","text":"elbo_batch(flow, logp, xs)\nelbo_batch([rng, ] flow, logp, n_samples)\n\nBatched ELBO estimates that transforms a matrix of samples (each column represents a single sample) in one call.  This is more efficient for invertible neural-network flows (RealNVP/NSF) as it leverages the batched operation of the neural networks.\n\nInputs\n\nflow::Bijectors.MultivariateTransformed\nlogp: function returning log-density of target\nxs or n_samples: column-wise sample batch or number of samples\n\nReturns\n\nScalar estimate of the ELBO\n\n\n\n\n\n","category":"function"},{"location":"api/#Log-likelihood","page":"API","title":"Log-likelihood","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"By maximizing the log-likelihood, it is equivalent to minimizing the forward KL divergence between q_theta and p:","category":"page"},{"location":"api/","page":"API","title":"API","text":"beginaligned\n min_theta mathbbE_pleftlog q_theta(Z)-log p(Z)right quad text(Forward KL) \n = max_theta mathbbE_pleftlog q_theta(Z)right quad text(Expected log-likelihood)\nendaligned","category":"page"},{"location":"api/","page":"API","title":"API","text":"Forward KL minimization is typically used for generative modeling when samples from p are given.","category":"page"},{"location":"api/#NormalizingFlows.loglikelihood","page":"API","title":"NormalizingFlows.loglikelihood","text":"loglikelihood(rng, flow::Bijectors.TransformedDistribution, xs::AbstractVecOrMat)\n\nCompute the log-likelihood for variational distribution flow at a batch of samples xs from  the target distribution p. \n\nArguments\n\nrng: random number generator (empty argument, only needed to ensure the same signature as other variational objectives)\nflow: variational distribution to be trained. In particular  \"flow = transformed(q₀, T::Bijectors.Bijector)\",  q₀ is a reference distribution that one can easily sample and compute logpdf\nxs: samples from the target distribution p.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training-Loop","page":"API","title":"Training Loop","text":"","category":"section"},{"location":"api/#NormalizingFlows.optimize","page":"API","title":"NormalizingFlows.optimize","text":"optimize(\n    ad::ADTypes.AbstractADType, \n    loss, \n    θ₀::AbstractVector{T}, \n    re, \n    args...; \n    kwargs...\n)\n\nIteratively updating the parameters θ of the normalizing flow re(θ) by calling grad!  and using the given optimiser to compute the steps.\n\nArguments\n\nad::ADTypes.AbstractADType: automatic differentiation backend\nloss: a general loss function θ -> loss(θ, args...) returning a scalar loss value that will be minimised\nθ₀::AbstractVector{T}: initial parameters for the loss function (in the context of normalizing flows, it will be the flattened flow parameters)\nre: reconstruction function that maps the flattened parameters to the normalizing flow\nargs...: additional arguments for loss (will be set as DI.Constant)\n\nKeyword Arguments\n\nmax_iters::Int=10000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nshow_progress::Bool=true: whether to show the progress bar. The default information printed in the progress bar is the iteration number, the loss value, and the gradient norm.\ncallback=nothing: callback function with signature cb(iter, opt_state, re, θ) which returns a dictionary-like object of statistics to be displayed in the progress bar. re and θ are used for reconstructing the normalizing flow in case that user  want to further axamine the status of the flow.\nhasconverged = (iter, opt_stats, re, θ, st) -> false: function that checks whether the training has converged. The default is to always return false.\nprog=ProgressMeter.Progress(           max_iters; desc=\"Training\", barlen=31, showspeed=true, enabled=show_progress       ): progress bar configuration\n\nReturns\n\nθ: trained parameters of the normalizing flow\nopt_stats: statistics of the optimiser\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"function"},{"location":"api/#Available-Flows","page":"API","title":"Available Flows","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"NormalizingFlows.jl provides two commonly used normalizing flows: RealNVP and  Neural Spline Flow (NSF).","category":"page"},{"location":"api/#RealNVP-(Affine-Coupling-Flow)","page":"API","title":"RealNVP (Affine Coupling Flow)","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"These helpers construct commonly used coupling-based flows with sensible defaults.","category":"page"},{"location":"api/#NormalizingFlows.realnvp","page":"API","title":"NormalizingFlows.realnvp","text":"realnvp(q0, hdims, nlayers; paramtype = Float64)\nrealnvp(q0; paramtype = Float64)\n\nConstruct a RealNVP flow by stacking nlayers RealNVP_layer blocks with odd–even masking. The 1-argument variant defaults to 10 layers with hidden sizes [32, 32] per conditioner.\n\nArguments\n\nq0::Distribution{Multivariate,Continuous}: base distribution (e.g. MvNormal(zeros(d), I)).\nhdims::AbstractVector{Int}: hidden sizes for the conditioner networks.\nnlayers::Int: number of stacked RealNVP layers.\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type (use Float32 for GPU friendliness).\n\nReturns\n\nBijectors.TransformedDistribution representing the RealNVP flow.\n\nExample\n\nq0 = MvNormal(zeros(2), I); flow = realnvp(q0, [64,64], 8)\nx = rand(flow, 128); lp = logpdf(flow, x)\n\n\n\n\n\nrealnvp(q0; paramtype = Float64)\n\nDefault constructor: 10 layers, each conditioner uses hidden sizes [32, 32]. Follows a common RealNVP architecture similar to Appendix E of [ASD2020].\n\n[ASD2020]: Agrawal, A., Sheldon, D., Domke, J. (2020). Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization. NeurIPS.\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.RealNVP_layer","page":"API","title":"NormalizingFlows.RealNVP_layer","text":"RealNVP_layer(dims, hdims; paramtype = Float64)\n\nConstruct a single RealNVP layer by composing two AffineCoupling bijectors with complementary odd–even masks.\n\nArguments\n\ndims::Int: dimensionality of the problem.\nhdims::AbstractVector{Int}: hidden sizes of the conditioner networks.\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type.\n\nReturns\n\nA Bijectors.Bijector representing the RealNVP layer.\n\nExample\n\nlayer = RealNVP_layer(4, [64, 64])\ny = layer(randn(4, 16))  # batched forward\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.AffineCoupling","page":"API","title":"NormalizingFlows.AffineCoupling","text":"AffineCoupling(dim, hdims, mask_idx, paramtype)\nAffineCoupling(dim, mask, s, t)\n\nAffine coupling bijector used in RealNVP [LJS2017].\n\nTwo subnetworks s (log-scale, exponentiated in the forward pass) and t (shift) act on one partition of the input, conditioned on the complementary partition (as defined by mask). For numerical stability, the output of s passes through tanh before exponentiation.\n\nArguments\n\ndim::Int: total dimensionality of the input.\nhdims::AbstractVector{Int}: hidden sizes for the conditioner MLPs s and t.\nmask_idx::AbstractVector{Int}: indices of the dimensions to transform. The complement is used as the conditioner input.\n\nKeyword Arguments\n\nparamtype::Type{<:AbstractFloat}: parameter element type (e.g. Float32).\n\nFields\n\nmask::Bijectors.PartitionMask: partition specification.\ns::Flux.Chain: conditioner producing log-scales for the transformed block.\nt::Flux.Chain: conditioner producing shifts for the transformed block.\n\nNotes\n\nForward: with (x₁,x₂,x₃) = partition(mask, x), compute y₁ = x₁ .* exp.(s(x₂)) .+ t(x₂).\nLog-determinant: sum(s(x₂)) (or columnwise for batched matrices).\nAll methods support both vectors and column-major batches (matrices).\n\n[LJS2017]: Dinh, L., Sohl-Dickstein, J. and Bengio, S. (2017).  Density estimation using Real NVP. ICLR.\n\n\n\n\n\n","category":"type"},{"location":"api/#Neural-Spline-Flow-(NSF)","page":"API","title":"Neural Spline Flow (NSF)","text":"","category":"section"},{"location":"api/#NormalizingFlows.nsf","page":"API","title":"NormalizingFlows.nsf","text":"nsf(q0, hdims, K, B, nlayers; paramtype = Float64)\nnsf(q0; paramtype = Float64)\n\nConstruct an NSF by stacking nlayers NSF_layer blocks. The one-argument variant defaults to 10 layers with [32, 32] hidden sizes, 10 knots, and boundary 30 (scaled by one(T)).\n\nArguments\n\nq0::Distribution{Multivariate,Continuous}: base distribution.\nhdims::AbstractVector{Int}: hidden sizes of the conditioner network.\nK::Int: spline knots per coordinate.\nB::AbstractFloat: spline boundary.\nnlayers::Int: number of NSF layers.\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type.\n\nReturns\n\nBijectors.TransformedDistribution representing the NSF flow.\n\nNotes:\n\nUnder the hood, nsf relies on the rational quadratic spline function implememented in \n\nMonotonicSplines.jl for performance reasons.  MonotonicSplines.jl uses  KernelAbstractions.jl to support batched operations.  Because of this, so far nsf only supports Zygote as the AD type.\n\nExample\n\nq0 = MvNormal(zeros(3), I); flow = nsf(q0, [64,64], 8, 3.0, 6)\nx = rand(flow, 128); lp = logpdf(flow, x)\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.NSF_layer","page":"API","title":"NormalizingFlows.NSF_layer","text":"NSF_layer(dims, hdims, K, B; paramtype = Float64)\n\nBuild a single Neural Spline Flow (NSF) layer by composing two NeuralSplineCoupling bijectors with complementary odd–even masks.\n\nArguments\n\ndims::Int: dimensionality of the problem.\nhdims::AbstractVector{Int}: hidden sizes of the conditioner network.\nK::Int: number of spline knots.\nB::AbstractFloat: spline boundary.\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type.\n\nReturns\n\nA Bijectors.Bijector representing the NSF layer.\n\nExample\n\nlayer = NSF_layer(4, [64,64], 10, 3.0)\ny = layer(randn(4, 32))\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.NeuralSplineCoupling","page":"API","title":"NormalizingFlows.NeuralSplineCoupling","text":"NeuralSplineCoupling(dim, hdims, K, B, mask_idx, paramtype)\nNeuralSplineCoupling(dim, K, n_dims_transferred, B, nn, mask)\n\nNeural Rational Quadratic Spline (RQS) coupling bijector [DBMP2019].\n\nA conditioner network takes the unchanged partition as input and outputs the parameters of monotonic rational quadratic splines for the transformed coordinates. Batched inputs (matrices with column vectors) are supported.\n\nArguments\n\ndim::Int: total input dimension.\nhdims::AbstractVector{Int}: hidden sizes for the conditioner MLP.\nK::Int: number of spline knots per transformed coordinate.\nB::AbstractFloat: boundary/box constraint for spline domain.\nmask_idx::AbstractVector{Int}: indices of the transformed coordinates.\n\nKeyword Arguments\n\nparamtype::Type{<:AbstractFloat}: parameter element type.\n\nFields\n\nnn::Flux.Chain: conditioner that outputs all spline params for all transformed dims.\nmask::Bijectors.PartitionMask: partition specification.\n\nNotes\n\nOutput dimensionality of the conditioner is (3K - 1) * n_transformed.\nFor computation performance, we rely on \n\nMonotonicSplines.jl for the building the rational quadratic spline functions.\n\nSee MonotonicSplines.rqs_forward and MonotonicSplines.rqs_inverse for forward/inverse \n\nand log-determinant computations.\n\n[DBMP2019]: Durkan, C., Bekasov, A., Murray, I. and Papamarkou, T. (2019). Neural Spline Flows. NeurIPS.\n\n\n\n\n\n","category":"type"},{"location":"api/#Utility-Functions","page":"API","title":"Utility Functions","text":"","category":"section"},{"location":"api/#NormalizingFlows.create_flow","page":"API","title":"NormalizingFlows.create_flow","text":"create_flow(layers, q0)\n\nConstruct a normalizing flow by composing the provided bijector layers and attaching them to the base distribution q0.\n\nlayers: an iterable of Bijectors.Bijector objects that are composed in order (left-to-right) via function composition.\nq0: the base distribution (e.g., MvNormal(zeros(d), I)).\n\nReturns a Bijectors.TransformedDistribution representing the resulting flow.\n\nExample\n\nusing Distributions\nq0 = MvNormal(zeros(2), I)\nflow = create_flow((Bijectors.Scale([1.0, 2.0]), Bijectors.Shift([0.0, 1.0])), q0)\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.fnn","page":"API","title":"NormalizingFlows.fnn","text":"fnn(\n    input_dim::Int,\n    hidden_dims::AbstractVector{Int},\n    output_dim::Int;\n    inlayer_activation=Flux.leakyrelu,\n    output_activation=nothing,\n    paramtype::Type{T} = Float64,\n)\n\nCreate a fully connected neural network (FNN).\n\nArguments\n\ninput_dim::Int: The dimension of the input layer.\nhidden_dims::AbstractVector{<:Int}: A vector of integers specifying the dimensions of the hidden layers.\noutput_dim::Int: The dimension of the output layer.\ninlayer_activation: The activation function for the hidden layers. Defaults to Flux.leakyrelu.\noutput_activation: The activation function for the output layer. Defaults to Flux.tanh.\nparamtype::Type{T} = Float64: The type of the parameters in the network, defaults to Float64.\n\nReturns\n\nA Flux.Chain representing the FNN.\n\n\n\n\n\n","category":"function"},{"location":"usage/#General-usage","page":"General usage","title":"General usage","text":"","category":"section"},{"location":"usage/","page":"General usage","title":"General usage","text":"train_flow is the main function to train a normalizing flow.  The users mostly need to specify a normalizing flow flow,  the variational objective vo and its corresponding arguments args....","category":"page"},{"location":"usage/#NormalizingFlows.train_flow","page":"General usage","title":"NormalizingFlows.train_flow","text":"train_flow([rng::AbstractRNG, ]vo, flow, args...; kwargs...)\n\nTrain the given normalizing flow flow by calling optimize.\n\nArguments\n\nrng::AbstractRNG: random number generator (default: Random.default_rng())\nvo: variational objective with signature vo(rng, flow, args...).    We implement elbo, elbo_batch, and loglikelihood.\nflow: the normalizing flow–-a Bijectors.TransformedDistribution (recommended)\nargs...: additional arguments passed to vo\n\nKeyword Arguments\n\nmax_iters::Int=1000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nADbackend::ADTypes.AbstractADType=ADTypes.AutoZygote():    automatic differentiation backend, currently supports   ADTypes.AutoZygote(), ADTypes.ForwardDiff(), ADTypes.ReverseDiff(),    ADTypes.AutoMooncake() and   ADTypes.AutoEnzyme(;       mode=Enzyme.set_runtime_activity(Enzyme.Reverse),       function_annotation=Enzyme.Const,   ).   If user wants to use AutoEnzyme, please make sure to include the set_runtime_activity and function_annotation as shown above.\nkwargs...: additional keyword arguments for optimize (See optimize for details)\n\nReturns\n\nflow_trained: trained normalizing flow\nopt_stats: statistics of the optimiser during the training process    (See optimize for details)\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"function"},{"location":"usage/","page":"General usage","title":"General usage","text":"The flow object can be constructed by transformed function in Bijectors.jl. For example, for mean-field Gaussian VI, we can construct the flow family as follows:","category":"page"},{"location":"usage/","page":"General usage","title":"General usage","text":"using Distributions, Bijectors\nT = Float32\n@leaf MvNormal # to prevent params in q₀ from being optimized\nq₀ = MvNormal(zeros(T, 2), ones(T, 2))\n# the flow family is defined by a shift and a scale \nflow = Bijectors.transformed(q₀, Bijectors.Shift(zeros(T,2)) ∘ Bijectors.Scale(ones(T, 2)))","category":"page"},{"location":"usage/","page":"General usage","title":"General usage","text":"To train the Gaussian VI targeting distribution p via ELBO maximization, run:","category":"page"},{"location":"usage/","page":"General usage","title":"General usage","text":"using NormalizingFlows, Optimisers\nusing ADTypes, Mooncake\n\nsample_per_iter = 10\nflow_trained, stats, _ = train_flow(\n    elbo,\n    flow,\n    logp,\n    sample_per_iter;\n    max_iters=5_000,\n    optimiser=Optimisers.Adam(one(T)/100),\n    ADbackend=ADTypes.AutoMooncake(; config=Mooncake.Config()),\n    show_progress=true,\n)","category":"page"},{"location":"RealNVP/#Demo-of-RealNVP-on-2D-Banana-Distribution","page":"RealNVP","title":"Demo of RealNVP on 2D Banana Distribution","text":"","category":"section"},{"location":"RealNVP/","page":"RealNVP","title":"RealNVP","text":"using Random, Distributions, LinearAlgebra\nusing Functors\nusing Optimisers, ADTypes\nusing Mooncake\nusing NormalizingFlows\n\n\ntarget = Banana(2, one(T), 100one(T))\nlogp = Base.Fix1(logpdf, target)\n\n######################################\n# set up the RealNVP\n######################################\n@leaf MvNormal\nq0 = MvNormal(zeros(T, 2), I)\n\nd = 2\nhdims = [16, 16]\nnlayers = 3\n\n# use NormalizingFlows.realnvp to create a RealNVP flow\nflow = realnvp(q0, hdims, nlayers; paramtype=T)\nflow_untrained = deepcopy(flow)\n\n\n######################################\n# start training\n######################################\nsample_per_iter = 16\n\n# callback function to log training progress\ncb(iter, opt_stats, re, θ) = (sample_per_iter=sample_per_iter,ad=adtype)\nadtype = ADTypes.AutoMooncake(; config = Mooncake.Config())\n\ncheckconv(iter, stat, re, θ, st) = stat.gradient_norm < one(T)/1000\nflow_trained, stats, _ = train_flow(\n    rng, \n    elbo,        # using elbo_batch instead of elbo achieves 4-5 times speedup \n    flow,\n    logp,\n    sample_per_iter;\n    max_iters=10,   # change to larger number of iterations (e.g., 50_000) for better results\n    optimiser=Optimisers.Adam(5e-4),\n    ADbackend=adtype,\n    show_progress=true,\n    callback=cb,\n    hasconverged=checkconv,\n)\nθ, re = Optimisers.destructure(flow_trained)\nlosses = map(x -> x.loss, stats)","category":"page"},{"location":"PlanarFlow/#Planar-Flow-on-a-2D-Banana-Distribution","page":"Planar Flow","title":"Planar Flow on a 2D Banana Distribution","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"This example demonstrates learning a synthetic 2D banana distribution with a planar normalizing flow [RM2015] by maximizing the Evidence Lower BOund (ELBO).","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"The two required ingredients are:","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"A log-density function logp for the target distribution.\nA parametrised invertible transformation (the planar flow) applied to a simple base distribution.","category":"page"},{"location":"PlanarFlow/#Target-Distribution","page":"Planar Flow","title":"Target Distribution","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"The banana target used here is defined in example/targets/banana.jl (see source for details):","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"using Random, Distributions\nRandom.seed!(123)\n\ntarget = Banana(2, 1.0, 10.0)  # (dimension, nonlinearity, scale)\nlogp = Base.Fix1(logpdf, target)","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"You can visualise its contour and samples (figure shipped as banana.png).","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"(Image: Banana)","category":"page"},{"location":"PlanarFlow/#Planar-Flow","page":"Planar Flow","title":"Planar Flow","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"A planar flow of length N applies a sequence of planar layers to a base distribution q₀:","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"T_ntheta_n(x) = x + u_n tanh(w_n^T x + b_n) qquad n = 1ldotsN","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"Parameters θₙ = (uₙ, wₙ, bₙ) are learned. Bijectors.jl provides PlanarLayer.","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"using Bijectors\nusing Functors # for @leaf\n\nfunction create_planar_flow(n_layers::Int, q₀)\n    d = length(q₀)\n    Ls = [PlanarLayer(d) for _ in 1:n_layers]\n    ts = reduce(∘, Ls)  # alternatively: FunctionChains.fchain(Ls)\n    return transformed(q₀, ts)\nend\n\n@leaf MvNormal  # prevent updating base distribution parameters\nq₀ = MvNormal(zeros(2), ones(2))\nflow = create_planar_flow(10, q₀)\nflow_untrained = deepcopy(flow)  # keep copy for comparison","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"If you build many layers (e.g. > ~30) you may reduce compilation time by using FunctionChains.jl:","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"# uncomment the following lines to use FunctionChains\n# using FunctionChains\n# ts = fchain([PlanarLayer(d) for _ in 1:n_layers])","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"See this comment for how the compilation time might be a concern.","category":"page"},{"location":"PlanarFlow/#Training-the-Flow","page":"Planar Flow","title":"Training the Flow","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"We maximize the ELBO (here using the minibatch estimator elbo_batch) with the generic train_flow interface.","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"using NormalizingFlows\nusing ADTypes, Optimisers\nusing Mooncake\n\nsample_per_iter = 32\nadtype = ADTypes.AutoMooncake(; config=Mooncake.Config())  # try AutoZygote() / AutoForwardDiff() / etc.\n# optional: callback function to track the batch size per iteration and the AD backend used \ncb(iter, opt_stats, re, θ) = (sample_per_iter=sample_per_iter, ad=adtype)\n# optional: defined stopping criteria when the gradient norm is less than 1e-3\ncheckconv(iter, stat, re, θ, st) = stat.gradient_norm < 1e-3\n\nflow_trained, stats, _ = train_flow(\n    elbo_batch,\n    flow,\n    logp,\n    sample_per_iter;\n    max_iters = 20_000,\n    optimiser = Optimisers.Adam(1e-2),\n    ADbackend = adtype,\n    callback = cb,\n    hasconverged = checkconv,\n    show_progress = false,\n)\n\nlosses = map(x -> x.loss, stats)","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"Plot the losses (negative ELBO):","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"using Plots\nplot(losses; xlabel = \"iteration\", ylabel = \"negative ELBO\", label = \"\", lw = 2)","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"(Image: elbo)","category":"page"},{"location":"PlanarFlow/#Evaluating-the-Trained-Flow","page":"Planar Flow","title":"Evaluating the Trained Flow","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"The trained flow is a Bijectors.TransformedDistribution, so we can call rand to draw iid samples and call logpdf to evaluate the log-density function of the flow. See documentation of Bijectors.jl for details.","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"n_samples = 1_000\nsamples_trained   = rand(flow_trained, n_samples)\nsamples_untrained = rand(flow_untrained, n_samples)\nsamples_true      = rand(target, n_samples)","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"Simple visual comparison:","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"using Plots\nscatter(samples_true[1, :], samples_true[2, :]; label=\"Target\", ms=2, alpha=0.5)\nscatter!(samples_untrained[1, :], samples_untrained[2, :]; label=\"Untrained\", ms=2, alpha=0.5)\nscatter!(samples_trained[1, :],  samples_trained[2, :];  label=\"Trained\", ms=2, alpha=0.5)\nplot!(title = \"Planar Flow: Before vs After Training\", xlabel = \"x₁\", ylabel = \"x₂\", legend = :topleft)","category":"page"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"(Image: compare)","category":"page"},{"location":"PlanarFlow/#Notes","page":"Planar Flow","title":"Notes","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"Use elbo instead of elbo_batch for a single-sample estimator.\nSwitch AD backends by changing adtype (see ADTypes.jl).\nMarking the base distribution with @leaf prevents its parameters from being updated during training.","category":"page"},{"location":"PlanarFlow/#Reference","page":"Planar Flow","title":"Reference","text":"","category":"section"},{"location":"PlanarFlow/","page":"Planar Flow","title":"Planar Flow","text":"[RM2015]: Rezende, D. & Mohamed, S. (2015). Variational Inference with Normalizing Flows. ICML.","category":"page"},{"location":"#NormalizingFlows.jl","page":"Home","title":"NormalizingFlows.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NormalizingFlows.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The purpose of this package is to provide a simple and flexible interface for  variational inference (VI) and normalizing flows (NF) for Bayesian computation and generative modeling. The key focus is to ensure modularity and extensibility, so that users can easily  construct (e.g., define customized flow layers) and combine various components  (e.g., choose different VI objectives or gradient estimates)  for variational approximation of general target distributions,  without being tied to specific probabilistic programming frameworks or applications. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, run the following command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]  # enter Pkg mode\n(@v1.11) pkg> add NormalizingFlows","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then simply run the following command to use the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NormalizingFlows","category":"page"},{"location":"#What-are-normalizing-flows?","page":"Home","title":"What are normalizing flows?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Normalizing flows transform a simple reference distribution q_0 (sometimes known as base distribution) to  a complex distribution q_theta using invertible functions with trainable parameter theta, aiming to approximate a target distribution p. The approximation is achieved by minimizing some statistical distances between q and p.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In more details, given the base distribution, usually a standard Gaussian distribution, i.e., q_0 = mathcalN(0 I), we apply a series of parameterized invertible transformations (called flow layers), T_1 theta_1 cdots T_N theta_k, yielding that","category":"page"},{"location":"","page":"Home","title":"Home","text":"Z_N = T_N theta_N circ cdots circ T_1 theta_1 (Z_0)  quad Z_0 sim q_0quad  Z_N sim q_theta ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where theta = (theta_1 dots theta_N) are the parameters to be learned, and q_theta is the transformed distribution (typically called the variational distribution or the flow distribution).  This describes sampling procedure of normalizing flows, which requires sending draws from the base distribution through a forward pass of these flow layers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since all the transformations are invertible (technically diffeomorphic),  we can evaluate the density of a normalizing flow distribution q_theta by the change of variable formula: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"q_theta(x)=fracq_0left(T_1^-1 circ cdots circ\nT_N^-1(x)right)prod_n=1^N J_nleft(T_n^-1 circ cdots circ\nT_N^-1(x)right) quad J_n(x)=leftoperatornamedet nabla_x\nT_n(x)right","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here we drop the subscript theta_n n = 1 dots N for simplicity.  Density evaluation of normalizing flow requires computing the inverse and the Jacobian determinant of each flow layer.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given the feasibility of i.i.d. sampling and density evaluation, normalizing flows can be trained by minimizing some statistical distances to the target distribution p. The typical choice of the statistical distance is the forward and reverse Kullback-Leibler (KL) divergence, which leads to the following optimization problems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\ntextReverse KLquad\nargmin _theta mathbbE_q_thetaleftlog q_theta(Z)-log p(Z)right \n= argmin _theta mathbbE_q_0leftlog fracq_theta(T_Ncirc cdots circ T_1(Z_0))p(T_Ncirc cdots circ T_1(Z_0))right \n= argmax _theta mathbbE_q_0left log pleft(T_N circ cdots circ T_1(Z_0)right)-log q_0(X)+sum_n=1^N log J_nleft(T_n circ cdots circ T_1(X)right)right\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"and ","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\ntextForward KLquad\nargmin _theta mathbbE_pleftlog q_theta(Z)-log p(Z)right \n= argmin _theta mathbbE_pleftlog q_theta(Z)right \nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"Both problems can be solved via standard stochastic optimization algorithms, such as stochastic gradient descent (SGD) and its variants. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a detailed introduction of normalizing flows, please refer to","category":"page"}]
}
