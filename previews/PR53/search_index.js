var documenterSearchIndex = {"docs":
[{"location":"customized_layer/#Defining-Your-Own-Flow-Layer","page":"Customize your own flow layer","title":"Defining Your Own Flow Layer","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"In practice, user might want to define their own normalizing flow.  As briefly noted in What are normalizing flows?, the key is to define a customized normalizing flow layer, including its transformation and inverse, as well as the log-determinant of the Jacobian of the transformation. Bijectors.jl offers a convenient interface to define a customized bijection. We refer users to the documentation of Bijectors.jl for more details. Flux.jl is also a useful package, offering a convenient interface to define neural networks.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"In this tutorial, we demonstrate how to define a customized normalizing flow layer – an Affine Coupling Layer (Dinh et al., 2016) – using Bijectors.jl and Flux.jl.","category":"page"},{"location":"customized_layer/#Affine-Coupling-Flow","page":"Customize your own flow layer","title":"Affine Coupling Flow","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Given an input vector boldsymbolx, the general coupling transformation splits it into two parts: boldsymbolx_I_1 and boldsymbolx_Isetminus I_1. Only one part (e.g., boldsymbolx_I_1) undergoes a bijective transformation f, noted as the coupling law,  based on the values of the other part (e.g., boldsymbolx_Isetminus I_1), which remains unchanged. ","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"beginarrayllll\nc_I_1(cdot  f theta)  mathbbR^d rightarrow mathbbR^d  c_I_1^-1(cdot  f theta)  mathbbR^d rightarrow mathbbR^d \n boldsymbolx_I backslash I_1 mapsto boldsymbolx_I backslash I_1   boldsymboly_I backslash I_1 mapsto boldsymboly_I backslash I_1 \n boldsymbolx_I_1 mapsto fleft(boldsymbolx_I_1  thetaleft(boldsymbolx_Isetminus I_1right)right)   boldsymboly_I_1 mapsto f^-1left(boldsymboly_I_1  thetaleft(boldsymboly_Isetminus I_1right)right)\nendarray","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Here theta can be an arbitrary function, e.g., a neural network. As long as f(cdot theta(boldsymbolx_Isetminus I_1)) is invertible, c_I_1 is invertible, and the  Jacobian determinant of c_I_1 is easy to compute:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"lefttextdet nabla_x c_I_1(x)right = lefttextdet nabla_x_I_1 f(x_I_1 theta(x_Isetminus I_1))right","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"The affine coupling layer is a special case of the coupling transformation, where the coupling law f is an affine function:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"beginaligned\nboldsymbolx_I_1 mapsto boldsymbolx_I_1 odot sleft(boldsymbolx_Isetminus I_1right) + tleft(boldsymbolx_I setminus I_1right) \nboldsymbolx_I backslash I_1 mapsto boldsymbolx_I backslash I_1\nendaligned","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Here, s and t are arbitrary functions (often neural networks) called the \"scaling\" and \"translation\" functions, respectively.  They produce vectors of the same dimension as boldsymbolx_I_1.","category":"page"},{"location":"customized_layer/#Implementing-Affine-Coupling-Layer","page":"Customize your own flow layer","title":"Implementing Affine Coupling Layer","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"We start by defining a simple 3-layer multi-layer perceptron (MLP) using Flux.jl,  which will be used to define the scaling s and translation functions t in the affine coupling layer.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Flux\n\nfunction MLP_3layer(input_dim::Int, hdims::Int, output_dim::Int; activation=Flux.leakyrelu)\n    return Chain(\n        Flux.Dense(input_dim, hdims, activation),\n        Flux.Dense(hdims, hdims, activation),\n        Flux.Dense(hdims, output_dim),\n    )\nend","category":"page"},{"location":"customized_layer/#Construct-the-Object","page":"Customize your own flow layer","title":"Construct the Object","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Following the user interface of Bijectors.jl, we define a struct AffineCoupling as a subtype of Bijectors.Bijector. The functions parition , combine are used to partition and recombine a vector into 3 disjoint subvectors.  And PartitionMask is used to store this partition rule.  These three functions are all defined in Bijectors.jl; see the documentaion for more details.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Functors\nusing Bijectors\nusing Bijectors: partition, combine, PartitionMask\n\nstruct AffineCoupling <: Bijectors.Bijector\n    dim::Int\n    mask::Bijectors.PartitionMask\n    s::Flux.Chain\n    t::Flux.Chain\nend\n\n# to apply functions to the parameters that are contained in AffineCoupling.s and AffineCoupling.t, \n# and to re-build the struct from the parameters, we use the functor interface of `Functors.jl` \n# see https://fluxml.ai/Flux.jl/stable/models/functors/#Functors.functor\n@functor AffineCoupling (s, t)\n\nfunction AffineCoupling(\n    dim::Int,  # dimension of input\n    hdims::Int, # dimension of hidden units for s and t\n    mask_idx::AbstractVector, # index of dimension that one wants to apply transformations on\n)\n    cdims = length(mask_idx) # dimension of parts used to construct coupling law\n    s = MLP_3layer(cdims, hdims, cdims)\n    t = MLP_3layer(cdims, hdims, cdims)\n    mask = PartitionMask(dim, mask_idx)\n    return AffineCoupling(dim, mask, s, t)\nend","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"By default, we define s and t using the MLP_3layer function, which is a 3-layer MLP with leaky ReLU activation function.","category":"page"},{"location":"customized_layer/#Implement-the-Forward-and-Inverse-Transformations","page":"Customize your own flow layer","title":"Implement the Forward and Inverse Transformations","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"function Bijectors.transform(af::AffineCoupling, x::AbstractVector)\n    # partition vector using 'af.mask::PartitionMask`\n    x₁, x₂, x₃ = partition(af.mask, x)\n    y₁ = x₁ .* af.s(x₂) .+ af.t(x₂)\n    return combine(af.mask, y₁, x₂, x₃)\nend\n\nfunction Bijectors.transform(iaf::Inverse{<:AffineCoupling}, y::AbstractVector)\n    af = iaf.orig\n    # partition vector using `af.mask::PartitionMask`\n    y_1, y_2, y_3 = partition(af.mask, y)\n    # inverse transformation\n    x_1 = (y_1 .- af.t(y_2)) ./ af.s(y_2)\n    return combine(af.mask, x_1, y_2, y_3)\nend","category":"page"},{"location":"customized_layer/#Implement-the-Log-determinant-of-the-Jacobian","page":"Customize your own flow layer","title":"Implement the Log-determinant of the Jacobian","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Notice that here we wrap the transformation and the log-determinant of the Jacobian into a single function, with_logabsdet_jacobian.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"function Bijectors.with_logabsdet_jacobian(af::AffineCoupling, x::AbstractVector)\n    x_1, x_2, x_3 = Bijectors.partition(af.mask, x)\n    y_1 = af.s(x_2) .* x_1 .+ af.t(x_2)\n    logjac = sum(log ∘ abs, af.s(x_2))\n    return combine(af.mask, y_1, x_2, x_3), logjac\nend\n\nfunction Bijectors.with_logabsdet_jacobian(\n    iaf::Inverse{<:AffineCoupling}, y::AbstractVector\n)\n    af = iaf.orig\n    # partition vector using `af.mask::PartitionMask`\n    y_1, y_2, y_3 = partition(af.mask, y)\n    # inverse transformation\n    x_1 = (y_1 .- af.t(y_2)) ./ af.s(y_2)\n    logjac = -sum(log ∘ abs, af.s(y_2))\n    return combine(af.mask, x_1, y_2, y_3), logjac\nend","category":"page"},{"location":"customized_layer/#Construct-Normalizing-Flow","page":"Customize your own flow layer","title":"Construct Normalizing Flow","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Now with all the above implementations, we are ready to use the AffineCoupling layer for normalizing flow  by applying it to a base distribution q_0.","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"using Random, Distributions, LinearAlgebra\ndim = 4\nhdims = 10\nLs = [\n    AffineCoupling(dim, hdims, 1:2), \n    AffineCoupling(dim, hdims, 3:4), \n    AffineCoupling(dim, hdims, 1:2), \n    AffineCoupling(dim, hdims, 3:4), \n    ]\nts = reduce(∘, Ls)\nq₀ = MvNormal(zeros(Float32, dim), I)\nflow = Bijectors.transformed(q₀, ts)","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"We can now sample from the flow:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"x = rand(flow, 10)","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"And evaluate the density of the flow:","category":"page"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"logpdf(flow, x[:,1])","category":"page"},{"location":"customized_layer/#Reference","page":"Customize your own flow layer","title":"Reference","text":"","category":"section"},{"location":"customized_layer/","page":"Customize your own flow layer","title":"Customize your own flow layer","text":"Dinh, L., Sohl-Dickstein, J. and Bengio, S., 2016. Density estimation using real nvp.  arXiv:1605.08803.","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#Main-Function","page":"API","title":"Main Function","text":"","category":"section"},{"location":"api/#NormalizingFlows.train_flow","page":"API","title":"NormalizingFlows.train_flow","text":"train_flow([rng::AbstractRNG, ]vo, flow, args...; kwargs...)\n\nTrain the given normalizing flow flow by calling optimize.\n\nArguments\n\nrng::AbstractRNG: random number generator (default: Random.default_rng())\nvo: objective with signature vo(rng, flow, args...)\nflow: a Bijectors.TransformedDistribution (recommended)\nargs...: additional arguments passed to vo\n\nKeyword Arguments\n\nmax_iters::Int=1000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nADbackend::ADTypes.AbstractADType=ADTypes.AutoZygote():    automatic differentiation backend, currently supports   ADTypes.AutoZygote(), ADTypes.ForwardDiff(), ADTypes.ReverseDiff(),    ADTypes.AutoMooncake() and   ADTypes.AutoEnzyme(;       mode=Enzyme.set_runtime_activity(Enzyme.Reverse),       function_annotation=Enzyme.Const,   ).   If user wants to use AutoEnzyme, please make sure to include the set_runtime_activity and function_annotation as shown above.\nkwargs...: additional keyword arguments for optimize (See optimize for details)\n\nReturns\n\nflow_trained: trained normalizing flow\nopt_stats: statistics of the optimiser during the training process    (See optimize for details)\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"The flow object can be constructed by transformed function in Bijectors.jl. For example, for Gaussian VI, we can construct the flow as follows:","category":"page"},{"location":"api/","page":"API","title":"API","text":"using Distributions, Bijectors\nT = Float32\n@leaf MvNormal # to prevent params in q₀ from being optimized\nq₀ = MvNormal(zeros(T, 2), ones(T, 2))\nflow = Bijectors.transformed(q₀, Bijectors.Shift(zeros(T,2)) ∘ Bijectors.Scale(ones(T, 2)))","category":"page"},{"location":"api/","page":"API","title":"API","text":"To train the Gaussian VI targeting distribution p via ELBO maximization, run:","category":"page"},{"location":"api/","page":"API","title":"API","text":"using NormalizingFlows, Optimisers\n\nsample_per_iter = 10\nflow_trained, stats, _ = train_flow(\n    elbo,\n    flow,\n    logp,\n    sample_per_iter;\n    max_iters = 2_000,\n    optimiser = Optimisers.ADAM(0.01 * one(T)),\n)","category":"page"},{"location":"api/#Coupling-based-flows-(default-constructors)","page":"API","title":"Coupling-based flows (default constructors)","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"These helpers construct commonly used coupling-based flows with sensible defaults.","category":"page"},{"location":"api/#NormalizingFlows.realnvp","page":"API","title":"NormalizingFlows.realnvp","text":"realnvp(q0, hdims, nlayers; paramtype = Float64)\nrealnvp(q0; paramtype = Float64)\n\nConstruct a RealNVP flow by stacking nlayers RealNVP_layer blocks with odd–even masking. The no-argument variant uses 10 layers with [32, 32] hidden sizes per coupling network.\n\nArguments\n\nq0::Distribution{Multivariate,Continuous}: base distribution (e.g. MvNormal(zeros(d), I))\nhdims::AbstractVector{Int}: hidden sizes for the s and t MLPs\nnlayers::Int: number of RealNVP layers\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type\n\nReturns\n\nBijectors.MultivariateTransformed representing the RealNVP flow\n\n\n\n\n\nrealnvp(q0; paramtype = Float64)\n\nDefault constructor of RealNVP with 10 layers,  each coupling function has 2 hidden layers with 32 units.  Following the general architecture as in the Apdx. E of [ASD2020].\n\n[ASD2020]: Agrawal, A., & Sheldon, D., & Domke, J. (2020). Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization. In NeurIPS.\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.nsf","page":"API","title":"NormalizingFlows.nsf","text":"nsf(q0, hdims, K, B, nlayers; paramtype = Float64)\n\nDefault constructor of Neural Spline Flow (NSF), which composes nlayers NSF_layer blocks with odd-even masking.\n\nArguments\n\nq0::Distribution{Multivariate,Continuous}: base distribution (e.g., MvNormal(zeros(d), I)).\nhdims::AbstractVector{Int}: hidden layer sizes of the coupling networks.\nK::Int: number of spline knots.\nB::AbstractFloat: boundary range for spline knots.\nnlayers::Int: number of NSF_layer blocks.\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type (e.g., Float32 for GPU-friendly).\n\nReturns\n\nBijectors.MultivariateTransformed representing the NSF flow.\n\nUse the shorthand nsf(q0) to construct a default configuration.\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.RealNVP_layer","page":"API","title":"NormalizingFlows.RealNVP_layer","text":"RealNVP_layer(dims, hdims; paramtype = Float64)\n\nConstruct a single RealNVP layer using two affine coupling bijections with odd–even masks.\n\nArguments\n\ndims::Int: dimensionality of the target distribution\nhdims::AbstractVector{Int}: hidden sizes for the s and t MLPs\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type\n\nReturns\n\nA Bijectors.Bijector representing the RealNVP layer\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.NSF_layer","page":"API","title":"NormalizingFlows.NSF_layer","text":"NSF_layer(dims, hdims, K, B; paramtype = Float64)\n\nDefault constructor of a single layer of Neural Spline Flow (NSF), which is a composition of two neural spline coupling transformations with complementary odd–even masks.\n\nArguments\n\ndims::Int: dimension of the problem\nhdims::AbstractVector{Int}: hidden sizes of the MLP used to parameterize the spline\nK::Int: number of knots for the rational quadratic spline\nB::AbstractFloat: boundary for the spline domain\n\nKeyword Arguments\n\nparamtype::Type{T} = Float64: parameter element type\n\nReturns\n\nA Bijectors.Bijector representing the NSF layer\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.AffineCoupling","page":"API","title":"NormalizingFlows.AffineCoupling","text":"Affine coupling layer used in RealNVP.\n\nImplements two subnetworks s (scale, exponentiated) and t (shift) applied to one partition of the input, conditioned on the complementary partition. The scale network uses tanh on its output before exponentiation to improve stability during training.\n\nSee also: Dinh et al., 2016 (RealNVP).\n\n\n\n\n\n","category":"type"},{"location":"api/#NormalizingFlows.NeuralSplineCoupling","page":"API","title":"NormalizingFlows.NeuralSplineCoupling","text":"Neural Rational Quadratic Spline Coupling layer \n\nReferences\n\n[1] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G., Neural Spline Flows, CoRR, arXiv:1906.04032 [stat.ML],  (2019). \n\n\n\n\n\n","category":"type"},{"location":"api/#NormalizingFlows.create_flow","page":"API","title":"NormalizingFlows.create_flow","text":"create_flow(layers, q0)\n\nConstruct a normalizing flow by composing the provided bijector layers and attaching them to the base distribution q0.\n\nlayers: an iterable of Bijectors.Bijector objects that are composed in order (left-to-right) via function composition.\nq0: the base distribution (e.g., MvNormal(zeros(d), I)).\n\nReturns a Bijectors.TransformedDistribution representing the resulting flow.\n\nExample\n\nusing Distributions\nq0 = MvNormal(zeros(2), I)\nflow = create_flow((Bijectors.Scale([1.0, 2.0]), Bijectors.Shift([0.0, 1.0])), q0)\n\n\n\n\n\n","category":"function"},{"location":"api/#Variational-Objectives","page":"API","title":"Variational Objectives","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"We provide ELBO (reverse KL) and expected log-likelihood (forward KL). You can also supply your own objective with the signature vo(rng, flow, args...).","category":"page"},{"location":"api/#Evidence-Lower-Bound-(ELBO)","page":"API","title":"Evidence Lower Bound (ELBO)","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"By maximizing the ELBO, it is equivalent to minimizing the reverse KL divergence between q_theta and p:","category":"page"},{"location":"api/","page":"API","title":"API","text":"beginaligned\nmin _theta mathbbE_q_thetaleftlog q_theta(Z)-log p(Z)right  quad text(Reverse KL)\n = max _theta mathbbE_q_0left log pleft(T_N circ cdots circ\nT_1(Z_0)right)-log q_0(X)+sum_n=1^N log J_nleft(T_n circ cdots circ\nT_1(X)right)right quad text(ELBO)\nendaligned","category":"page"},{"location":"api/","page":"API","title":"API","text":"Reverse KL minimization is typically used for Bayesian computation when only logp is available.","category":"page"},{"location":"api/#NormalizingFlows.elbo","page":"API","title":"NormalizingFlows.elbo","text":"elbo(flow, logp, xs)\nelbo([rng, ] flow, logp, n_samples)\n\nMonte Carlo estimates of the ELBO from a batch of samples xs from the  reference distribution flow.dist.\n\nArguments\n\nrng: random number generator\nflow: variational distribution to be trained. In particular  flow = transformed(q₀, T::Bijectors.Bijector),  q₀ is a reference distribution that one can easily sample and compute logpdf\nlogp: log-pdf of the target distribution (not necessarily normalized)\nxs: samples from reference dist q₀\nn_samples: number of samples from reference dist q₀\n\n\n\n\n\n","category":"function"},{"location":"api/#NormalizingFlows.elbo_batch","page":"API","title":"NormalizingFlows.elbo_batch","text":"elbo_batch(flow, logp, xs)\nelbo_batch([rng, ] flow, logp, n_samples)\n\nBatched ELBO estimates that transforms a matrix of samples (each column represents a single sample) in one call.  This is more efficient for invertible neural-network flows (RealNVP/NSF) as it leverages the batched operation of the neural networks.\n\nInputs\n\nflow::Bijectors.MultivariateTransformed\nlogp: function returning log-density of target\nxs or n_samples: column-wise sample batch or number of samples\n\nReturns\n\nScalar estimate of the ELBO\n\n\n\n\n\n","category":"function"},{"location":"api/#Log-likelihood","page":"API","title":"Log-likelihood","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"By maximizing the log-likelihood, it is equivalent to minimizing the forward KL divergence between q_theta and p:","category":"page"},{"location":"api/","page":"API","title":"API","text":"beginaligned\n min_theta mathbbE_pleftlog q_theta(Z)-log p(Z)right quad text(Forward KL) \n = max_theta mathbbE_pleftlog q_theta(Z)right quad text(Expected log-likelihood)\nendaligned","category":"page"},{"location":"api/","page":"API","title":"API","text":"Forward KL minimization is typically used for generative modeling when samples from p are given.","category":"page"},{"location":"api/#NormalizingFlows.loglikelihood","page":"API","title":"NormalizingFlows.loglikelihood","text":"loglikelihood(rng, flow::Bijectors.TransformedDistribution, xs::AbstractVecOrMat)\n\nCompute the log-likelihood for variational distribution flow at a batch of samples xs from  the target distribution p. \n\nArguments\n\nrng: random number generator (empty argument, only needed to ensure the same signature as other variational objectives)\nflow: variational distribution to be trained. In particular  \"flow = transformed(q₀, T::Bijectors.Bijector)\",  q₀ is a reference distribution that one can easily sample and compute logpdf\nxs: samples from the target distribution p.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training-Loop","page":"API","title":"Training Loop","text":"","category":"section"},{"location":"api/#NormalizingFlows.optimize","page":"API","title":"NormalizingFlows.optimize","text":"optimize(\n    ad::ADTypes.AbstractADType, \n    loss, \n    θ₀::AbstractVector{T}, \n    re, \n    args...; \n    kwargs...\n)\n\nIteratively updating the parameters θ of the normalizing flow re(θ) by calling grad!  and using the given optimiser to compute the steps.\n\nArguments\n\nad::ADTypes.AbstractADType: automatic differentiation backend\nloss: a general loss function θ -> loss(θ, args...) returning a scalar loss value that will be minimised\nθ₀::AbstractVector{T}: initial parameters for the loss function (in the context of normalizing flows, it will be the flattened flow parameters)\nre: reconstruction function that maps the flattened parameters to the normalizing flow\nargs...: additional arguments for loss (will be set as DI.Constant)\n\nKeyword Arguments\n\nmax_iters::Int=10000: maximum number of iterations\noptimiser::Optimisers.AbstractRule=Optimisers.ADAM(): optimiser to compute the steps\nshow_progress::Bool=true: whether to show the progress bar. The default information printed in the progress bar is the iteration number, the loss value, and the gradient norm.\ncallback=nothing: callback function with signature cb(iter, opt_state, re, θ) which returns a dictionary-like object of statistics to be displayed in the progress bar. re and θ are used for reconstructing the normalizing flow in case that user  want to further axamine the status of the flow.\nhasconverged = (iter, opt_stats, re, θ, st) -> false: function that checks whether the training has converged. The default is to always return false.\nprog=ProgressMeter.Progress(           max_iters; desc=\"Training\", barlen=31, showspeed=true, enabled=show_progress       ): progress bar configuration\n\nReturns\n\nθ: trained parameters of the normalizing flow\nopt_stats: statistics of the optimiser\nst: optimiser state for potential continuation of training\n\n\n\n\n\n","category":"function"},{"location":"#NormalizingFlows.jl","page":"Home","title":"NormalizingFlows.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NormalizingFlows.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The purpose of this package is to provide a simple and flexible interface for  variational inference (VI) and normalizing flows (NF) for Bayesian computation and generative modeling. The key focus is to ensure modularity and extensibility, so that users can easily  construct (e.g., define customized flow layers) and combine various components  (e.g., choose different VI objectives or gradient estimates)  for variational approximation of general target distributions,  without being tied to specific probabilistic programming frameworks or applications. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, run the following command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]  # enter Pkg mode\n(@v1.11) pkg> add git@github.com:TuringLang/NormalizingFlows.jl.git","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then simply run the following command to use the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NormalizingFlows","category":"page"},{"location":"#What-are-normalizing-flows?","page":"Home","title":"What are normalizing flows?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Normalizing flows transform a simple reference distribution q_0 (sometimes known as base distribution) to  a complex distribution q_theta using invertible functions with trainable parameter theta, aiming to approximate a target distribution p. The approximation is achieved by minimizing some statistical distances between q and p.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In more details, given the base distribution, usually a standard Gaussian distribution, i.e., q_0 = mathcalN(0 I), we apply a series of parameterized invertible transformations (called flow layers), T_1 theta_1 cdots T_N theta_k, yielding that","category":"page"},{"location":"","page":"Home","title":"Home","text":"Z_N = T_N theta_N circ cdots circ T_1 theta_1 (Z_0)  quad Z_0 sim q_0quad  Z_N sim q_theta ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where theta = (theta_1 dots theta_N) are the parameters to be learned, and q_theta is the transformed distribution (typically called the variational distribution or the flow distribution).  This describes sampling procedure of normalizing flows, which requires sending draws from the base distribution through a forward pass of these flow layers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since all the transformations are invertible (technically diffeomorphic),  we can evaluate the density of a normalizing flow distribution q_theta by the change of variable formula: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"q_theta(x)=fracq_0left(T_1^-1 circ cdots circ\nT_N^-1(x)right)prod_n=1^N J_nleft(T_n^-1 circ cdots circ\nT_N^-1(x)right) quad J_n(x)=leftoperatornamedet nabla_x\nT_n(x)right","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here we drop the subscript theta_n n = 1 dots N for simplicity.  Density evaluation of normalizing flow requires computing the inverse and the Jacobian determinant of each flow layer.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given the feasibility of i.i.d. sampling and density evaluation, normalizing flows can be trained by minimizing some statistical distances to the target distribution p. The typical choice of the statistical distance is the forward and reverse Kullback-Leibler (KL) divergence, which leads to the following optimization problems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\ntextReverse KLquad\nargmin _theta mathbbE_q_thetaleftlog q_theta(Z)-log p(Z)right \n= argmin _theta mathbbE_q_0leftlog fracq_theta(T_Ncirc cdots circ T_1(Z_0))p(T_Ncirc cdots circ T_1(Z_0))right \n= argmax _theta mathbbE_q_0left log pleft(T_N circ cdots circ T_1(Z_0)right)-log q_0(X)+sum_n=1^N log J_nleft(T_n circ cdots circ T_1(X)right)right\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"and ","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\ntextForward KLquad\nargmin _theta mathbbE_pleftlog q_theta(Z)-log p(Z)right \n= argmin _theta mathbbE_pleftlog q_theta(Z)right \nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"Both problems can be solved via standard stochastic optimization algorithms, such as stochastic gradient descent (SGD) and its variants. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a detailed introduction of normalizing flows, please refer to","category":"page"},{"location":"example/#Example:-Using-Planar-Flow","page":"Example","title":"Example: Using Planar Flow","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Here we provide a minimal demonstration of learning a synthetic 2D banana distribution using planar flows (Rezende and Mohamed, 2015) by maximizing the ELBO. To complete this task, the two key inputs are:","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"the log-density function of the target distribution, \nthe planar flow. \nthe log-density function of the target distribution\nthe planar flow","category":"page"},{"location":"example/#The-Target-Distribution","page":"Example","title":"The Target Distribution","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The Banana object is defined in example/targets/banana.jl (see the source for details).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"p = Banana(2, 1.0f-1, 100.0f0)\nlogp = Base.Fix1(logpdf, p)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Visualize the contour of the log-density and the sample scatters of the target distribution:","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: Banana)","category":"page"},{"location":"example/#The-Planar-Flow","page":"Example","title":"The Planar Flow","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The planar flow is defined by repeatedly applying a sequence of invertible transformations to a base distribution q_0. The building blocks for a planar flow of length N are the following invertible transformations, called planar layers:","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"T_n theta_n(x)=x+u_n cdot tanh left(w_n^T x+b_nright) quad n=1 ldots N","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Here theta_n = (u_n w_n b_n) n=1 dots N are the parameters to be learned. Bijectors.jl provides PlanarLayer(). Below is a 20-layer planar flow on a 2D standard Gaussian base distribution.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Bijectors, FunctionChains\nusing Functors\n\nfunction create_planar_flow(n_layers::Int, q₀)\n    d = length(q₀)\n    Ls = [f32(PlanarLayer(d)) for _ in 1:n_layers]\n    ts = fchain(Ls)\n    return transformed(q₀, ts)\nend\n\n# create a 20-layer planar flow\n@leaf MvNormal # to prevent params in q₀ from being optimized\nq₀ = MvNormal(zeros(Float32, 2), I)\nflow = create_planar_flow(20, q₀)\nflow_untrained = deepcopy(flow) # keep a copy of the untrained flow for comparison","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Notice: Using fchain (FunctionChains.jl) reduces compilation time versus chaining with ∘ for many layers.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"ts = reduce(∘, [f32(PlanarLayer(d)) for i in 1:20]) ","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"However, we recommend using fchain to reduce the compilation time when the number of layers is large. See this comment for how the compilation time might be a concern.","category":"page"},{"location":"example/#Flow-Training","page":"Example","title":"Flow Training","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Then we can train the flow by maximizing the ELBO using the train_flow function as follows: ","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using NormalizingFlows\nusing ADTypes\nusing Optimisers\n\nsample_per_iter = 10\n# callback function to track the number of samples used per iteration\ncb(iter, opt_stats, re, θ) = (sample_per_iter=sample_per_iter,)\n# defined stopping criteria when the gradient norm is less than 1e-3\ncheckconv(iter, stat, re, θ, st) = stat.gradient_norm < 1e-3\nflow_trained, stats, _ = train_flow(\n    elbo,\n    flow,\n    logp,\n    sample_per_iter;\n    max_iters=200_00,\n    optimiser=Optimisers.ADAM(),\n    callback=cb,\n    hasconverged=checkconv,\n    ADbackend=AutoZygote(), # using Zygote as the AD backend\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Examine the loss values during training:","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Plots\n\nlosses = map(x -> x.loss, stats)\nplot(losses; xlabel = \"#iteration\", ylabel= \"negative ELBO\", label=\"\", linewidth=2) ","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: elbo)","category":"page"},{"location":"example/#Evaluating-Trained-Flow","page":"Example","title":"Evaluating Trained Flow","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we can evaluate the trained flow by sampling from it and compare it with the target distribution. Since the flow is defined as a Bijectors.TransformedDistribution, one can easily sample from it using rand function, or examine the density using logpdf function. See documentation of Bijectors.jl for details.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Random, Distributions\n\nnsample = 1000\nsamples_trained = rand(flow_trained, n_samples) # 1000 iid samples from the trained flow \nsamples_untrained = rand(flow_untrained, n_samples) # 1000 iid samples from the untrained flow\nsamples_true = rand(p, n_samples) # 1000 iid samples from the target\n\n# plot \nscatter(samples_true[1, :], samples_true[2, :]; label=\"True Distribution\", color=:blue, markersize=2, alpha=0.5)\nscatter!(samples_untrained[1, :], samples_untrained[2, :]; label=\"Untrained Flow\", color=:red, markersize=2, alpha=0.5)\nscatter!(samples_trained[1, :], samples_trained[2, :]; label=\"Trained Flow\", color=:green, markersize=2, alpha=0.5)\nplot!(title = \"Comparison of Trained and Untrained Flow\", xlabel = \"X\", ylabel= \"Y\", legend=:topleft) ","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Image: compare)","category":"page"},{"location":"example/#Reference","page":"Example","title":"Reference","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Rezende, D. and Mohamed, S., 2015. Variational inference with normalizing flows. International Conference on Machine Learning  ","category":"page"}]
}
